{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lenet-5 (1998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 아키텍쳐의 할아버지급의 위상을 가지는 Lenet-5 모델에 대해 알아보고자 한다. 이 모델이 소개된 논문의 주요 내용은 잘 설계된 신경망에 대한 효율성에 대한 입증과 Graph Transformer Network으로 Lenet은 하나의 성공적인 예시로 어찌보면 조연처럼 나오지만, 그래도 우리가 관심있는 것은 Lenet모델이니 이 글에서는 Lenet에만 집중하록 하겠다.\n",
    "\n",
    "> 논문 : [GradientBased Learning Applied to Document Recognition [1998]](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅰ. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "better pattern recognition system can be built by relying more on automatic learning and less on hand-designed heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/Lenet/traditional_pattern_recognition.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당시 패턴 인식 문제에서 가장 보편적인 방법은 사람이 직접 설계한 모듈을 통해 Feature vector를 추출하여 이를 입력으로 분류기를 훈련시키는 방식이였다.\n",
    "이러한 방식에는 크게 두가지의 문제점이 있었는데,  \n",
    "1. 모듈 설계 자체의 어려움과,\n",
    "2. 다른 task마다 매번 새로운 모듈을 설계해야 한다는 점이다.\n",
    "이러한 문제점에도 불구하고 이러한 방식을 썼던 이유는 분류기가 낮은 차원의 입력값에서만 성능을 보였기 때문이다.\n",
    "\n",
    "하지만 다음과 같은 3가지의 변화로 상황이 바뀌었따\n",
    "1. 컴퓨터의 계산 성능의 향상\n",
    "2. 더 많은 수의 데이터를 사용할 수 있게 되었다.\n",
    "3. 머신러닝 기법들의 발전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅱ. Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature extractor에 neural network를 사용하지 않았던 이유? Linear Layer의 경우 이미지의 input이 몇백에 달하기 때문에 매우 많은 수의 parameter가 필요하다. 이런 많은 parameter를 잘 학습시키기 위해서는 더 큰 training dataset이 필요하다. 또한 하드웨어의 메모리 용량에 제한에 걸릴 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 입문자라면 한번씩 써보게 되는 MNIST 데이터셋에 CNN 구조를 성공적으로 접목시킨 첫 모델이다. input data에서 사람이 직접 feature를 추출하여 모델에 넣기보다 raw 한 데이터를 넣었다. data에 대한 최소한의 지식이 있는 신경망을 사용해야 한다. 가장 중요한 문제점은 input의 translation이나 distortion에 취약할 수 있다는 점ㅁ이다.(학습이 될지언정 오래 걸릴 것이다) convolution network는 이러한 취약점을 태생적으로 극복할 수 있게 설계되었다.  \n",
    "두번째로 linear layer는 input의 위상적 특성을 완전히 무시한다. CNN은 receptive field에서만 connection 가지게 강제시킨다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Layer의 문제점 : parmeter가 너무 많다. - Generalize가 잘 안된다.\n",
    "CNN의 경우: local connection만 갖게 제한함, shared weight를 가짐 => feature map을 여러개 생성\n",
    "\n",
    "<img src=\"img/Lenet/convolution.png\" width=50% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution Layer는 shift, scale, distortion에 내성을 갖기 위해 크게 3가지의 발상에서 설계되었다.\n",
    "1. local receptive field\n",
    "2. shared weights\n",
    "3. sub-sampling\n",
    "이러한 개념을 적용하면 input의 모든 receptive field에 shared weight를 적용한 연산을 하면 feature map을 얻을 수 있다. 완전한 convolution layer는 각기 다른 weight로 여러 feature map을 추출한다.\n",
    "위의 예시에서 feature map의 하나의 unit은 input의 5x5 receptive field에서 25개의 weight(그리고 1개의 bias)로 얻은 연산의 결과인 것이다. 이를 한칸씩 이동시키면서 전체 feature map을 얻을 수 있다.(이 과정에서 전 영역과 겹치는 구간이 생긴다). 모든 영역에서 같은 weightd와 bias 사용하므로 모든 영역에서 같은 특징을 추출한다. 다른 weight를 사용하여 다른 특징을 담은 feature map을 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한번 특징을 추출하게 되면 그 위치적 정보의 중요성은 떨어진다. 예를 들어 왼쪽 위에서 꼭지점을 특징이 발견되었으면 0이 아닐것이라고 추측할 수 있다. 그 꼭지점의 정확한 위치는 크게 중요하지 않다.(오히려 해가 될 수 있다. 같은 숫자에서도 다르므로). feature map의 정보를 떨어트리는 방법으로 해상도를 낮추는 것이 있고 이는 sub-sampling layer로 이러한 목적을 달성할 수 있다. 이 layer는 영역의 모든 값들의 평균값을 구하여 weight를 곱하고 bias를 더한다음에 sigmoid function을 통과한다.(현재의 자주 쓰이는 parameter가 없는 pooling layer랑 다르다). 해상도를 낮추는 것이 목표이기 때문에 convolution layer와 달리 영역간 overlap이 일어나지 않고 결과적으로 너비와 높이가 1/2이 된다(2x2커널일시).\n",
    "통상적으로 전체 신경망은 convolution layer와 subsampling layer를 하나씩 반복하면서 이루어진다. 이러한 망을 통과하면서 spatial resolution이 낮은 많은 feature를 얻을 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅲ. Lenet-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/Lenet/lenet.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lenet-5는 총 7개의 trainable한 parameter를 가진 layer로 구성되어 있다.\n",
    "\n",
    "### 1. input\n",
    "\n",
    "32x32의 크기로 이는 원본데이터의 크기인 28x28보다 살짝 큰데 그 이유는 항상 특징이 최대한 receptive field 중앙에 놓여있기 바라기 때문에 그렇다고 한다. 평균값이 0, 표준편차가 1이 되도록 normalize를 하였다. (대략 바탕이 -0.1, 글씨가 1.175 의 값을 갖는다)\n",
    "\n",
    "### 2. C1\n",
    "\n",
    "5x5 커널를 사용하여 6개의 28x28의 feature map을 가지는 convolution layer이다. parameter 수= (5x5 + 1) x 6 = 156\n",
    "\n",
    "### 3. S1\n",
    "2x2 커널을 사용하여 6개의 28x28 feature map을 14x14로 축소시킨다. parameter 수= (1+1) x 6 = 12\n",
    "\n",
    "### 4. C3\n",
    "\n",
    "기본적으로 C1(우리가 흔히 사용하는 Convolution)과 같지만 6개의 채널을 모두 사용하지 않고 subset만을 사용하여 feature map을 생성한다.\n",
    "16개의 feature map에 대해서 사용하는 input channel은 밑의 표에서 확인할 수 있다.\n",
    "\n",
    "<img src='img/Lenet/c3.png' />\n",
    "\n",
    "왜 모든 channel를 사용하지 않는가라는 의문이 생길 수 있다. 논문에서는 그 이유로 2가지를 설명하는데\n",
    "1. 첫째로, connection의 수와 parameter의 수를 줄일 수 있다.\n",
    "2. 망의 대칭성을 무너트릴 수 있다. 이로써 생성되는 각 feature map이 최대한 다른 특징을 갖도록 유도할 수 있다.\n",
    "\n",
    "parameter 수 = (5*5*3+ 1)*6 + (5*5*4+1)*9 + (5*5*6+1)*1 = 1516\n",
    "\n",
    "### 5. S4\n",
    "\n",
    "C3의 feature map의 높이와 너비를 절반으로 줄인다\n",
    "\n",
    "### 6. C5\n",
    "\n",
    "S4에서 나온 feature map의 크기가 5x5이기 때문에 5x5 convolution을 하게 되면 1x1 feature map이 나온다. 따라서 이 연산은 Fully connected layer와 다름이 없다.\n",
    "\n",
    "parameter 수 = (5*5*16 + 1) *120 = 48120\n",
    "\n",
    "### 7. F6\n",
    "\n",
    "86개의 output을 가지는 linear layer\n",
    "\n",
    "parameter 수 = (120 + 1) x 84 = 10164\n",
    "\n",
    "\n",
    "### 8. Output\n",
    "\n",
    "\n",
    "\n",
    "###  Squashing Function\n",
    "\n",
    "y = Atanh(Sx) A = 1.7159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10164"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "121*84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48120"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5*5*16 + 1) *120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16x16 centered in 28x28 -1~1 1channel grayscaled image\n",
    "output = 10 sized vector +1 for correct and -1 for others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전형적인 Conv1-Pool1-Conv2-Pool2-Linear 구조이다.\n",
    "Convolution layer 의 경우 5*5 stride 1이며\n",
    "Average Pooling layer 의 경우 2*2 stride 2를 사용한다. 논문 내용을 보면 Pooling Layer에도 weight 1개 bias 1개의 파라미터가 있다고 한다.\n",
    "Conv2 의 같은 경우 feature map을 생성할 때 모든 channel을 사용하지 않고 지정된 channel들만 선택적으로 사용하여 feature map을 얻는데 그 내용은 다음과 같다\n",
    "\n",
    "<img src='img/Lenet/connection.png' />\n",
    "\n",
    "총 12개의 feature map을 얻는데 첫번째는 1번째 채널만 사용하고 두번째 feature map은 1번째와 2번쨰 채널에 커널을 적용하여 얻는 그런 식이다. 이러한 짓을 하는 이유를 2가지르 들었는데 하나는 parameter 수를 줄일 수 있다는 장점이 있고 두번째는 대칭성을 깨기 위해서, 즉 최대한 feauture map들이 서로 다른 특징을 담도록 유도하였다는 이유이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Lambda, Pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda tensor: 2 * tensor - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='~/git/cnn/data/', train=True, transform=transform)\n",
    "test_dataset = MNIST(root='~/git/cnn/data/', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img = train_dataset[0][0].squeeze(0)\n",
    "sample_img.shape\n",
    "plt.imshow(sample_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9840"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7291+2549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2707"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "217-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "257-217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(train_dataset, torch.randperm(len(train_dataset))[:9840])\n",
    "test_dataset = Subset(test_dataset, torch.randperm(len(train_dataset))[:2707])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PartialConv2d, self).__init__()\n",
    "        self.subsets = [[0], [0,1], [0,1], [1], [0,1], [0,1],\n",
    "                        [2], [2,3], [2,3], [3], [2,3], [2,3]]\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(len(subset), 1, 5) for subset in self.subsets])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feature_maps = []\n",
    "        for conv, subset in zip(self.convs, self.subsets):\n",
    "            inp = x[:, subset]\n",
    "            feature_maps.append(conv(inp))\n",
    "        return torch.cat(feature_maps, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channel, kernel_size):\n",
    "        super(AvgPool, self).__init__()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size)\n",
    "        self.weight = torch.nn.Parameter(torch.ones(input_channel, 1, 1, dtype=torch.float32))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(input_channel, 1, 1, dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.weight * self.avgpool(x) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Lenet1, self).__init__()\n",
    "        A = 1.7159\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            AvgPool(4, 2),\n",
    "            nn.Tanh(),\n",
    "            PartialConv2d(),\n",
    "            nn.Tanh(),\n",
    "            AvgPool(12, 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4*4*12, 10),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2578"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for i, param in lenet1.named_parameters():\n",
    "    total_parameters += param.numel()\n",
    "    \n",
    "total_parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Lenet1                                   --                        --\n",
       "├─Sequential: 1                          --                        --\n",
       "│    └─PartialConv2d: 2                  --                        --\n",
       "│    │    └─ModuleList: 3-1              --                        512\n",
       "├─Sequential: 1-1                        [1, 10]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 4, 24, 24]            104\n",
       "│    └─AvgPool: 2-2                      [1, 4, 12, 12]            --\n",
       "│    │    └─AvgPool2d: 3-2               [1, 4, 12, 12]            --\n",
       "│    └─Tanh: 2-3                         [1, 4, 12, 12]            --\n",
       "│    └─PartialConv2d: 2-4                [1, 12, 8, 8]             --\n",
       "│    └─AvgPool: 2-5                      [1, 12, 4, 4]             --\n",
       "│    │    └─AvgPool2d: 3-3               [1, 12, 4, 4]             --\n",
       "│    └─Tanh: 2-6                         [1, 12, 4, 4]             --\n",
       "│    └─Flatten: 2-7                      [1, 192]                  --\n",
       "│    └─Linear: 2-8                       [1, 10]                   1,930\n",
       "│    └─Tanh: 2-9                         [1, 10]                   --\n",
       "==========================================================================================\n",
       "Total params: 2,546\n",
       "Trainable params: 2,546\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.09\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.04\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(lenet1, (1,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'Tanh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-2596eaa80f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlenet1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLenet1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlenet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmilestones\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-236-3feb7ad727b1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m         self.net = nn.Sequential(\n\u001b[1;32m      7\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mAvgPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'Tanh'"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 50\n",
    "\n",
    "lenet1 = Lenet1().to(device)\n",
    "optimizer = torch.optim.SGD(lenet1.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15], gamma=0.2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss_tot, acc = 0, 0\n",
    "    for X, label in train_dataloader:\n",
    "        X, label = X.to(device), label.to(device)\n",
    "        pred_y = lenet1(X)\n",
    "        y = torch.full((len(label), 10), -1, dtype=torch.float32, device=device)\n",
    "        y.scatter_(1, label.unsqueeze(1), 1)\n",
    "        loss = loss_fn(pred_y, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_tot += loss.item()\n",
    "        pred_label = (pred_y).argmax(dim=1)\n",
    "        acc += (pred_label == label).sum().item() / len(label)\n",
    "    #scheduler.step()\n",
    "    loss_test, acc_test =  0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, label in test_dataloader:\n",
    "            X, label = X.to(device), label.to(device)\n",
    "            pred_y = lenet1(X)\n",
    "            y = torch.full((len(label), 10), -1, dtype=torch.float32, device=device)\n",
    "            y.scatter_(1, label.unsqueeze(1), 1)\n",
    "            loss = loss_fn(pred_y, y)\n",
    "            loss_test += loss.item()\n",
    "            pred_label = (pred_y).argmax(dim=1)\n",
    "            acc_test += (pred_label == label).sum().item() / len(label)\n",
    "    \n",
    "    print(f'epoch:{epoch}')\n",
    "    print(f'  Train metrics : loss:{loss_tot/len(train_dataloader):.4f}, acc:{acc/len(train_dataloader):.4f}')\n",
    "    print(f'  Test  metrics : loss:{loss_test/len(test_dataloader):.4f}, acc:{acc_test/len(test_dataloader):.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
